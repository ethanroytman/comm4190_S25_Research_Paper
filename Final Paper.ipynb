{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01822333-5610-47e9-b894-d1209b3a7d04",
   "metadata": {},
   "source": [
    "# Large Language Models in Telehealth: Applications, Benefits, and Prompt Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bf695-dc56-4b1d-8aed-cbdc9fd67c47",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Telehealth has rapidly expanded in recent years, providing remote medical consultations and patient care through digital communication. In parallel, large language models (LLMs) like GPT-3.5 and GPT-4 have demonstrated remarkable abilities to generate human-like text and engage in complex dialogues. These models are now being explored as tools to enhance telehealth services. Early studies and reviews highlight a range of potential telehealth applications for LLMs, from answering patient questions to translating medical information and assisting with clinical documentation (Busch et al., 2025). The appeal of LLMs in healthcare lies in their capacity to process vast medical knowledge and converse naturally, which could support both patients and providers in virtual care settings. This paper will aim to provide an overview of current and emerging telehealth use cases for LLMs, examine specific areas where LLMs could be especially effective, and evaluate the benefits and limitations of their use. In the second half, I will present example prompt strategies aimed at improving LLM performance in key telehealth functions such as patient intake, symptom triage, and documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70f988-b2e7-42a5-be5c-15c50407c371",
   "metadata": {},
   "source": [
    "## Telehealth Use Cases for Large Language Models\n",
    "\n",
    "### Patient Triage and Symptom Assessment\n",
    "\n",
    "One prominent telehealth application of LLMs is in patient triage and initial symptom assessment. Patient triage involves assessing patients’ conditions and ranking them in order of urgency, ensuring those with the most critical needs receive care first.  In this context, an LLM-driven chatbot can interact with patients to gather symptom details and urgency cues, guiding them on whether to seek emergency care, urgent clinic evaluation, or self-care at home. This can improve upon and partially automate the triage process that nurses or triage call centers traditionally handle. Early research is promising: for example, a recent study in an emergency department setting found that GPT-4 (as used in ChatGPT) achieved substantial agreement with professional triage decisions (κ ≈ 0.67), a performance on par with untrained junior physicians (Masanneck et al., 2024). This suggests that advanced LLMs can interpret clinical vignettes (brief patient case descriptions) and prioritize patients in a manner roughly equivalent to novice clinicians. Such triage chatbots could be available 24/7, improving access by handling high volumes of inquiries and potentially reducing unnecessary ER visits by advising appropriate alternatives. Indeed, LLMs tend to err on the side of caution by over-triaging (assigning slightly higher urgency), which may be safer in doubtful cases (Masanneck et al., 2024).\n",
    "\n",
    "However, there are potential limitations that reduce these benefits. Current LLMs can sometimes misinterpret symptoms or fail to ask critical follow-up questions, leading to incorrect health assessments. In triage, false negatives (underestimating severity) are especially dangerous as an LLM that advises a patient to stay home when they have a true emergency could cause harm. Studies have noted that while GPT-4 improved on GPT-3.5 in triage accuracy, neither consistently outperformed experienced human triage nurses,  suggesting that medical officials are still a leg up on LLMs (Masanneck et al., 2024). Therefore, LLM-based triage systems today are typically used as decision support rather than standalone decision-makers. Ethical considerations must also be taken into account when using LLMs for triage. Proper use should make the patients aware of the AI’s nature and ensure that clear disclaimers and safety nets are in place (for example, always instructing the patient to seek care if certain “red flag” symptoms are present, even if the AI doesn’t flag them). Overall, the future for LLMs in patient triage looks promising as it has the potential to increase the efficiency and access of triage, but further development should focus on increasing the accuracy of these models, especially regarding false negatives.\n",
    "\n",
    "### Chronic Disease Monitoring and Management\n",
    "\n",
    "LLMs can also play a role in the ongoing monitoring of chronic diseases via telehealth. Patients with conditions such as diabetes, hypertension, or asthma often require frequent check-ins that could be facilitated by an AI assistant. Research indicates that LLMs demonstrate feasible performance across a spectrum of chronic disease management activities. A 2025 systematic review found that various LLMs (including general models and those augmented with medical retrieval) were able to generate relevant, comprehensible, and accurate health recommendations about 71% of the time on average (Li et al., 2025). These models have been used to increase patient awareness of their conditions, reinforce preventive measures, and promote self-management behaviors (for example, encouraging a patient with hypertension to reduce salt intake or reminding a diabetic patient to check blood glucose). Indeed, a study comparing a conventional search-engine-based (Bing) to an LLM (ChatGPT) found that LLMs provided more accurate and reproducible results for home hypertension treatment, showing potential for LLMs to outpace conventional search engines in chronic treatment (Sidharthan, 24). In addition to factual support, LLM chatbots can offer motivational messages and empathetic responses that help patients feel emotionally supported. Notably, LLMs have been observed providing compassionate encouragement and fostering a sense of social connection for patients managing long-term illnesses (Li et al., 2025). This kind of always-available “health coach” could reduce the burden on clinical staff by handling routine queries and education, while also empowering patients to take an active role in their care.\n",
    "\n",
    "Despite these opportunities, significant challenges exist in using LLMs for chronic disease management. Current models struggle with advanced clinical tasks; for example, adjusting medication doses, handling complex comorbidities (managing multiple health problems at once), or making definitive diagnoses are beyond the safe capabilities of a general LLM without physician input (Li et al., 2025). If a patient reports new symptoms or side effects, an LLM might provide general advice, but it cannot substitute for a clinician’s judgment on changing a treatment plan. Thus, while LLMs can be a crucial tool in the everyday management of Chronic diseases, they cannot be a substitute for a physician altogether. \n",
    "\n",
    "### Clinical Documentation Assistance\n",
    "\n",
    "Telehealth sessions, like in-person visits, generate a need for clinical documentation, meaning physicians must write notes, summaries, or referrals based on each encounter. LLMs are increasingly being deployed to assist with or automate parts of this clinical documentation process. In a telehealth scenario, an LLM might listen (via transcript) to a live patient-provider conversation and then produce a draft of the encounter note, including key history, exam findings, and the plan of care. This use case is exemplified by products like Nuance’s Dragon Ambient eXperience (DAX) or Abridge, which leverage generative AI to create draft clinical notes from audio. Early pilot data from healthcare systems suggest significant benefits. For example, Sutter Health in California piloted an ambient documentation tool (Abridge) across hundreds of clinicians in 2024: 78% of clinicians reported improved job satisfaction and reduced workload when using the AI to draft notes, and nearly 60% felt the quality of their notes actually improved (Dugas, 25). By offloading the clerical task of typing up notes, doctors could focus more on the patient during the visit. Indeed, over half of the clinicians said they were able to give more undivided attention to patients when the AI assistant was active  (Dugas, 25). This enhanced focus can translate to better patient experience and communication. Additionally, these AI-generated notes can be made patient-friendly: the same pilot effort found that providing patients with an easy-to-read summary of the visit (produced by the AI from the note) helped patients recall their treatment plan and follow recommendations (Dugas, 25). Thus, LLMs in documentation not only save clinicians time but can also double as a tool for patient education by simplifying the medical record content for patients.\n",
    "\n",
    "Despite these recorded advantages, the limitations and risks of AI-assisted documentation must be carefully managed. A chief concern is accuracy and completeness: an LLM might include information that was not actually stated, omit critical nuances, or misattribute statements (for instance, assigning a symptom to the wrong timeframe). Such errors in a medical note could propagate mistakes in future care if not caught. For this reason, current deployments keep the clinician in the loop – the physician must review and edit the AI’s draft, as noted in the Sutter pilot, where the draft note is stored in the EHR for physician approval  (Dugas, 25). Overall, clinical documentation is one of the most effective telehealth use cases for LLMs and one that addresses a well-recognized pain point (clinician paperwork burden). Early results show that, when implemented responsibly, LLM-driven documentation can increase efficiency, reduce burnout, and potentially improve documentation quality, all of which benefit telehealth delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e3da70-a337-408b-95be-573d72d0b7f5",
   "metadata": {},
   "source": [
    "## Prompt Strategies to Enhance LLM Performance in Telehealth\n",
    "\n",
    "Effective deployment of LLMs in telehealth not only involves choosing the right applications but also crafting prompts that guide the model to produce useful and safe outputs. Prompt engineering, the design of the input instructions given to an LLM, has a significant impact on the quality and reliability of the model’s responses (Shah et al., 2024).  In clinical settings, well-structured prompts can help ensure the AI stays on track (following medical guidelines or conversational norms), asks pertinent questions, and presents information clearly. Below, I will present example prompt strategies for several key telehealth functions. These strategies illustrate how to maximize the LLM’s performance by providing role context, structured guidance, and clarity about the task at hand. Each example is tailored to a real-world use case (patient intake, symptom checking, and documentation), demonstrating an approach to prompt design that aligns with telehealth needs.\n",
    "\n",
    "### Prompt Strategy for Patient Intake and History Gathering\n",
    "\n",
    "In this use case,  an LLM acts as a medical assistant to collect a patient’s history before they speak with a human clinician. The goal is to ensure all relevant information is gathered in a patient-friendly yet thorough manner. An example prompt to achieve this goal can be found below.\n",
    "\n",
    "*Example Prompt: “You are a virtual intake nurse conducting a pre-appointment interview. Begin by greeting the patient and asking what brings them in (their main health concern). Then, for each symptom they mention, ask detailed follow-up questions (onset, duration, intensity, etc.). Next, ask about their past medical history, any chronic conditions, current medications, and any allergies. Be sure to ask one question at a time, using simple language, and wait for the patient’s answer. Show empathy and understanding in your responses (e.g., ‘I’m sorry to hear that, it sounds painful.’). Continue until you have gathered all relevant information. Finally, provide a brief summary of the key points: the chief complaint, important medical history, and any red-flag symptoms.”*\n",
    "\n",
    "This structured prompt ensures the LLM covers all essential intake elements in a logical flow. By scripting the interview structure, we reduce the chance that the AI will miss critical questions (like medication allergies) or get sidetracked. Role designation and empathy cues help maintain a professional and reassuring tone, which is important for patient comfort during telehealth intake. The summary at the end serves as an automatic note for the clinician and a check for completeness. Such a prompt aligns the LLM’s output with clinical expectations, effectively turning the model into a guided questionnaire that still feels conversational. Well-designed intake prompts can thus enhance LLM performance by making the interaction both comprehensive and patient-friendly, which is crucial in potentially addressing the limitations of data gathering and telehealth triage discussed above (Shah et al., 2024).\n",
    "\n",
    "### Prompt Strategy for Symptom Checking and Triage Advice\n",
    "\n",
    "This user scenario is similar to the one above, except it delves into actual triage. Specifically, an LLM is used to assess a patient’s symptoms and give preliminary advice on what to do next (for example, self-care, schedule a telehealth visit, or go to the ER). This is essentially a symptom checker chatbot scenario in telehealth.\n",
    "\n",
    "*Example Prompt: “You are a virtual triage assistant. A patient will describe their symptoms. Step 1: Analyze the symptoms and list a few possible explanations or diagnoses. Check if any symptoms sound like an emergency (for example: severe chest pain, trouble breathing, confusion, severe bleeding, etc.). Step 2: Based on this analysis, respond to the patient with clear advice on what to do next. If it’s likely minor or routine, advise home care or a regular doctor’s appointment. If it’s urgent, urge them to seek immediate medical attention. Begin by acknowledging the symptoms and concern. Use a reassuring tone and avoid causing panic, but be direct if the situation seems serious. Provide a brief rationale if appropriate (e.g., ‘Because you have had a high fever for three days...’). End by encouraging them to follow up if things worsen or if they are unsure.”*\n",
    "\n",
    "This prompt strategy guides the LLM to emulate a clinician’s thought process in triage: consider worst-case scenarios first, then the most likely explanations, and then match the outcome with the proper care setting. By structuring the prompt with an explicit reasoning step, we harness the model’s ability to internally weigh options (chain-of-thought), which has been shown to improve the correctness of answers in complex tasks (Sivarajkumar et al., 24). The model is instructed to be cautious (mention emergencies when relevant) and to align with triage best practices (implicitly referencing guideline criteria). The tone remains calm and empathetic, which is important so as not to unduly alarm the patient or, conversely, to ensure the patient takes urgent advice seriously when given. Such a prompt can help produce more reliable triage outcomes, for instance, making sure that a report of “crushing chest pain” is never met with a simple home care advice. Instead, the model will have “thought through” that this is a red flag and will output a recommendation to call emergency services. In testing scenarios, prompts that enforced this kind of systematic check have led to safer recommendations (Masanneck et al., 2024). Therefore, careful prompt engineering in symptom checking not only improves the accuracy of the advice but also embeds an extra layer of safety into the LLM’s telehealth role.\n",
    "\n",
    "### Prompt Strategy for Clinical Documentation Assistance\n",
    "\n",
    "This prompting would be used when an LLM is tasked with generating a draft clinical note or summary from a telehealth consultation (e.g., transforming a transcript of a video visit or chat into a structured medical note). The objective is to have a coherent, well-organized note that the clinician can quickly review and sign off.\n",
    "\n",
    "*Example Prompt: “You are a medical scribe AI assisting with documentation. Below is a transcript of a telehealth visit. Please summarize it into a structured medical note. Include the following sections: 1) Chief Complaint, 2) History of Present Illness (patient’s statements about symptoms and history in narrative form), 3) Relevant Past History (including medications and allergies if mentioned), 4) Assessment/Diagnosis (doctor’s assessment or working diagnosis), and 5) Plan (the doctor’s recommendations or follow-up plan). Use proper medical terminology and complete sentences. Important: Only include information that was explicitly stated in the transcript. Do not assume or create additional details. Maintain an objective tone. End the note with the doctor’s name and the date. Now, here is the transcript: [Insert patient-doctor dialogue].”*\n",
    "\n",
    "By giving the LLM a clear template and rules, we greatly improve the reliability of the generated documentation. The structured sections ensure that the note is organized in a familiar way for clinicians, which makes review faster. The caution against adding unspoken information is a safeguard against LLMs' tendency to fill gaps with content that was not in the original transcription. Real-world implementations have taken a similar approach: for example, Abridge’s system uses AI to identify key elements from conversation and slot them into a note format, which clinicians found improved note quality and completeness (Dugas, 25). The prompt above mirrors that process in a simplified manner. Additionally, specifying the tone (formal, objective) keeps the note professional; we don’t want the note to say “the patient’s cough was super bad” but rather “the patient reports a severe cough for 5 days.” If the transcript includes small talk or irrelevant details, the prompt doesn’t explicitly mention them, so the model will likely omit such content, focusing only on medically relevant information. These added details in prompting this ‘medical scribe’ LLM are crucial as a properly prompted LLM can streamline telehealth documentation without sacrificing accuracy or thoroughness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddba559-f983-49af-80aa-e4a1dc7b6fcb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Large language models hold significant promise for enhancing telehealth across a variety of applications, from patient interactions to documentation. LLMs like ChatGPT have already been tested in roles such as virtual triage assistants, chronic disease coaches, and clinical scribes. In each domain, they offer unique advantages: faster and potentially more individualized advice, continuous patient support between visits, and alleviating the administrative workload borne by physicians. Early results from studies and pilots are encouraging; for example, GPT-based systems are approaching nurse-level triage accuracy, and health systems have been reporting reduced clinician burnout with AI documentation support like Abridge. While these benefits are promising, it is also important to note the limitations of these models. Accuracy errors, potential biases, and inconsistent information gathering can all pose severe potential risks to patients. Thus, LLMs in telehealth should function as tools that support healthcare providers and patients, rather than independent decision-makers, and should have consistent physician oversight. Crucially, the performance of LLMs in telehealth is not solely reliant on the models themselves but also on how we use them. Thoughtfully engineered prompts, as demonstrated in the second half of this report, can improve the effectiveness and safety of LLM outputs in clinical contexts, potentially addressing the limitations discussed in this paper. For instance, a well-designed prompt can allow an LLM to have a thorough triage session with a patient or summarize a telehealth session without omitting key information. As both the models and our methods of guiding them improve, LLMs have the potential not just to support telehealth but to fundamentally reshape it into a more responsive, effective, and patient-centered system of care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
